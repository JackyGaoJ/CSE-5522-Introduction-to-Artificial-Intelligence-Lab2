{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "TweetUrl='https://github.com/aasiaeet/cse5522data/raw/master/db3_final_clean.csv'\n",
    "tweet_dataframe=pd.read_csv(TweetUrl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = 'red'>Part1</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordDict maps words to id\n",
    "# X is the document-word matrix holding the presence/absence of words in each tweet\n",
    "wordDict = {}\n",
    "idCounter = 0\n",
    "for i in range(tweet_dataframe.shape[0]):\n",
    "    allWords = tweet_dataframe.iloc[i,1].split(\" \")\n",
    "    for word in allWords:\n",
    "        if word not in wordDict:\n",
    "            wordDict[word] = idCounter\n",
    "            idCounter += 1\n",
    "X = np.zeros((tweet_dataframe.shape[0], idCounter),dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(tweet_dataframe.shape[0]):\n",
    "    allWords = tweet_dataframe.iloc[i,1].split(\" \")\n",
    "    for word in allWords:\n",
    "        X[i, wordDict[word]]  = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(tweet_dataframe.iloc[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numNeg = np.where(y > 0)[0][0] - 1\n",
    "numNeg = np.sum(y<=0) #More robust and, IMHO, easier to understand\n",
    "numPos = len(y) - numNeg\n",
    "probNeg = numNeg / (numNeg + numPos)\n",
    "probPos = 1 - probNeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute three distributions (four variables):\n",
    "def compute_distros(x,y):\n",
    "    # probWordGivenPositive: P(word|Sentiment = +ive)\n",
    "    probWordGivenPositive=np.sum(x[y>=0,:],axis=0) #Sum each word (column) to count how many times each word shows up (in positive examples)\n",
    "    probWordGivenPositive=probWordGivenPositive/np.sum(y>=0) #Divide by total number of (positive) examples to give distribution\n",
    "\n",
    "    # probWordGivenNegative: P(word|Sentiment = -ive)\n",
    "    probWordGivenNegative=np.sum(x[y<0,:],axis=0)\n",
    "    probWordGivenNegative=probWordGivenNegative/np.sum(y<0)\n",
    "\n",
    "    # priorPositive: P(Sentiment = +ive)\n",
    "    priorPositive = np.sum(y>=0)/y.shape[0] #Number of positive examples vs. all examples\n",
    "    # priorNegative: P(Sentiment = -ive)\n",
    "    priorNegative = 1 - priorPositive\n",
    "    # (note these last two form one distribution)\n",
    "\n",
    "    return probWordGivenPositive, probWordGivenNegative, priorPositive, priorNegative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute the following:\n",
    "# logProbWordPresentGivenPositive\n",
    "# logProbWordAbsentGivenPositive\n",
    "# logProbWordPresentGivenNegative\n",
    "# logProbWordAbsentGivenNegative\n",
    "# logPriorPositive\n",
    "# logPriorNegative\n",
    "def compute_logdistros(distros, min_prob):\n",
    "    #Assume missing words are simply very rare\n",
    "    #So, assign minimum probability to very small elements (e.g. 0 elements)\n",
    "    distros=np.where(distros>=min_prob,distros,min_prob)\n",
    "    #Also need to consider minimum probability for \"not\" distribution\n",
    "    distros=np.where(distros<=(1-min_prob),distros,1-min_prob)\n",
    "\n",
    "    #Note: Another option is to set the log for missing words to 0\n",
    "    #      This is equivalent to simply ignoring the word (since logP==0 is the same as P==1)\n",
    "\n",
    "    return np.log(distros), np.log(1-distros)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifyNB: \n",
    "#   words - vector of words of the tweet (binary vector)\n",
    "#   logProbWordPresentGivenPositive - log P(x_j = 1|+)\n",
    "#   logProbWordAbsentGivenPositive  - log P(x_j = 0|+)\n",
    "#   logProbWordPresentGivenNegative - log P(x_j = 1|-)\n",
    "#   logProbWordAbsentGivenNegative  - log P(x_j = 0|-)\n",
    "#   logPriorPositive - log P(+)\n",
    "#   logPriorNegative - log P(-)\n",
    "#   returns (label of x according to the NB classification rule, confidence about the label)\n",
    "\n",
    "# Note: you can also change the function definition if you wish to encapsulate all six log probs\n",
    "# as one model; just make sure to follow through below\n",
    "\n",
    "def classify_NB(words,logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive, \n",
    "               logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative, \n",
    "               logPriorPositive, logPriorNegative, ignore_absent):\n",
    "    if ignore_absent:\n",
    "        posSum = logPriorPositive + words.dot(logProbWordPresentGivenPositive)\n",
    "        negSum = logPriorNegative + words.dot(logProbWordPresentGivenNegative)\n",
    "    else:\n",
    "        posSum = logPriorPositive + np.sum(logProbWordPresentGivenPositive[words == 1, ]) + np.sum(logProbWordAbsentGivenPositive[words == 0, ])\n",
    "        negSum = logPriorNegative + np.sum(logProbWordPresentGivenNegative[words == 1, ]) + np.sum(logProbWordAbsentGivenNegative[words == 0, ])\n",
    "    return 1 if posSum>negSum else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testNB: Classify all xTest\n",
    "#   xTest - test data features\n",
    "#   yTest - true label of test data\n",
    "#   logProbWordPresentGivenPositive - log P(x_j = 1|+)\n",
    "#   logProbWordAbsentGivenPositive  - log P(x_j = 0|+)\n",
    "#   logProbWordPresentGivenNegative - log P(x_j = 1|-)\n",
    "#   logProbWordAbsentGivenNegative  - log P(x_j = 0|-)\n",
    "#   logPriorPositive - log P(+)\n",
    "#   logPriorNegative - log P(-)\n",
    "#   returns Average test error\n",
    "def test_NB(xTest, yTest, \n",
    "           logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive, \n",
    "           logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative, \n",
    "           logPriorPositive, logPriorNegative, ignore_absent):\n",
    "    error = 0\n",
    "    for i in range(xTest.shape[0]):\n",
    "        prediction = classify_NB(xTest[i, ],logProbWordPresentGivenPositive,\n",
    "                                logProbWordAbsentGivenPositive,logProbWordPresentGivenNegative, \n",
    "                                logProbWordAbsentGivenNegative,logPriorPositive, logPriorNegative, ignore_absent)\n",
    "        if (prediction!=yTest[i]):\n",
    "            error += 1\n",
    "    return error/xTest.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute accuracy ignoring absent words and incorporating absent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy when ignore absent words is : 0.827027027027027\n",
      "Average Accuracy when incoporating absent words is : 0.8162162162162162\n"
     ]
    }
   ],
   "source": [
    "total_error_ignore_absent = []\n",
    "total_error_incorporate_absent = []\n",
    "seed_of_split= 42 #Grader could change this value for training/test set randomization\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size = 0.2, random_state = seed_of_split)\n",
    "probWordGivenPositive, probWordGivenNegative, priorPositive, priorNegative = compute_distros(xTrain,yTrain)\n",
    "min_prob = 1/yTrain.shape[0] #Assume very rare words only appeared once\n",
    "logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive = compute_logdistros(probWordGivenPositive,min_prob)\n",
    "logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative = compute_logdistros(probWordGivenNegative,min_prob)\n",
    "logPriorPositive, logPriorNegative = compute_logdistros(priorPositive,min_prob)\n",
    "total_error_ignore_absent.append(test_NB(xTest, yTest, \n",
    "                                       logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive, \n",
    "                                       logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative, \n",
    "                                       logPriorPositive, logPriorNegative, ignore_absent = True))\n",
    "total_error_incorporate_absent.append(test_NB(xTest, yTest, \n",
    "                                       logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive, \n",
    "                                       logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative, \n",
    "                                       logPriorPositive, logPriorNegative, ignore_absent = False))\n",
    "\n",
    "print (\"Average Accuracy when ignore absent words is : \" + str(1-np.mean(total_error_ignore_absent)))\n",
    "print (\"Average Accuracy when incoporating absent words is : \" + str(1-np.mean(total_error_incorporate_absent)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report For Part1\n",
    "    When ignoring absent words and only calculating values for words present in the tweet, the accuracy is higher than incorporting absent words in a tweet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = 'red'>Part 2 Sticky Terms</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion to calculate PMI according to top numbers of sticky terms\n",
    "def calculate_PMI(tweet_dataframe, top_number):\n",
    "    w1_and_w2 = {}\n",
    "    words = {}\n",
    "    PMI = {}\n",
    "\n",
    "    for i in range(tweet_dataframe.shape[0]):\n",
    "        allWords = tweet_dataframe.iloc[i,1].split(\" \")\n",
    "        for index in range(len(allWords)):\n",
    "            word = allWords[index]\n",
    "            if word in words:\n",
    "                words[word] += 1\n",
    "            else:\n",
    "                words[word] = 1\n",
    "            if index < len(allWords)-1:\n",
    "                word_pre = allWords[index]\n",
    "                word_post = allWords[index+1]\n",
    "                if (word_pre,word_post) in w1_and_w2:\n",
    "                    w1_and_w2[word_pre,word_post] +=1\n",
    "                else:\n",
    "                    w1_and_w2[word_pre,word_post] = 1\n",
    "    w1_and_w2= dict(sorted(w1_and_w2.items(), key=lambda x: x[1], reverse=True))\n",
    "    words = dict(sorted(words.items(), key=lambda x:x[1], reverse = True))\n",
    "        \n",
    "    for (word_pre, word_post) in w1_and_w2:\n",
    "        probability_sticky  = w1_and_w2[word_pre, word_post] / sum(w1_and_w2.values())\n",
    "        probability_word_pre = words[word_pre] / sum(words.values())\n",
    "        probability_word_post = words[word_post] / sum(words.values())\n",
    "        PMI[word_pre, word_post] = probability_sticky/ (probability_word_pre * probability_word_post)\n",
    "    PMI = sorted(PMI.items(), key=lambda x:x[1], reverse = True)\n",
    "    \n",
    "    return dict(PMI[0:top_number])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of the Naive Bayes Model\n",
    "def calculate_accuracy(tweet_dataframe,PMI):\n",
    "    # wordDict maps words to id\n",
    "    # X is the document-word matrix holding the presence/absence of words in each tweet\n",
    "    wordDict = {}\n",
    "    idCounter = 0\n",
    "    for i in range(tweet_dataframe.shape[0]):\n",
    "        allWords = tweet_dataframe.iloc[i,1].split(\" \")\n",
    "        for word in allWords:\n",
    "            if word not in wordDict:\n",
    "                wordDict[word] = idCounter\n",
    "                idCounter += 1\n",
    "        for index in range(len(allWords)-1):\n",
    "            word_pre = allWords[index]\n",
    "            word_post = allWords[index+1]\n",
    "            if (word_pre,word_post) in PMI.keys():\n",
    "                if (word_pre,word_post) not in wordDict:\n",
    "                    wordDict[word_pre, word_post] = idCounter\n",
    "                    idCounter += 1\n",
    "\n",
    "    X = np.zeros((tweet_dataframe.shape[0], idCounter),dtype='float')\n",
    "    for i in range(tweet_dataframe.shape[0]):\n",
    "        allWords = tweet_dataframe.iloc[i,1].split(\" \")\n",
    "        for index in range(len(allWords)-1):\n",
    "            word_pre = allWords[index]\n",
    "            word_post = allWords[index+1]\n",
    "            X[i, wordDict[word_pre]]  = 1\n",
    "            if (word_pre,word_post) in PMI.keys():\n",
    "                 X[i, wordDict[word_pre, word_post]]  = 1\n",
    "\n",
    "    total_error_ignore_absent = []\n",
    "    total_error_incorporate_absent = []\n",
    "    seed_of_split = 42 # Grader can change this value for training/test set randomization\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size = 0.2,random_state = seed_of_split)\n",
    "    probWordGivenPositive, probWordGivenNegative, priorPositive, priorNegative = compute_distros(xTrain,yTrain)\n",
    "    min_prob = 1/yTrain.shape[0] #Assume very rare words only appeared once\n",
    "    logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive = compute_logdistros(probWordGivenPositive,min_prob)\n",
    "    logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative = compute_logdistros(probWordGivenNegative,min_prob)\n",
    "    logPriorPositive, logPriorNegative = compute_logdistros(priorPositive,min_prob)\n",
    "    total_error_ignore_absent.append(test_NB(xTest, yTest, \n",
    "                                           logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive, \n",
    "                                           logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative, \n",
    "                                           logPriorPositive, logPriorNegative, ignore_absent = True))\n",
    "    total_error_incorporate_absent.append(test_NB(xTest, yTest, \n",
    "                                           logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive, \n",
    "                                           logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative, \n",
    "                                           logPriorPositive, logPriorNegative, ignore_absent = False))\n",
    "    return [total_error_ignore_absent,total_error_incorporate_absent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get top <font color =\"red\">100 </font>PMI of sticky terms and accuracy after adding them as additional features in Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 sticky terms: \n",
      "('sb', 'xlv')\n",
      "('mormon', 'heroin')\n",
      "('board', 'retreat')\n",
      "('listing', 'appt')\n",
      "('gurrl', 'lemme')\n",
      "('hats', 'coats')\n",
      "('fuckkkk', 'thaaaa')\n",
      "('\\nboo', 'ed')\n",
      "('pegged', 'wannanewcar')\n",
      "('wannanewcar', 'sorryxterra')\n",
      "('subra', 'inet')\n",
      "('grrrrrrrrr', 'lazyass')\n",
      "('buzzzzing', 'battleoffife')\n",
      "('childrens', 'parties')\n",
      "('parties', 'difficult')\n",
      "('mankato', 'benefiting')\n",
      "('benefiting', 'leep')\n",
      "('comparison', 'goodbyebluesky')\n",
      "('total', 'horseshit')\n",
      "('electrical', 'appliances')\n",
      "('easily', 'amused')\n",
      "('amused', 'perhaps')\n",
      "('perhaps', 'bite')\n",
      "('siesta', 'key')\n",
      "('key', 'headn')\n",
      "('effing', 'snowflakes')\n",
      "('fort', 'collins')\n",
      "('conditioner', 'kicks')\n",
      "('latest', 'piano')\n",
      "('piano', 'composition')\n",
      "('homie', 'tj')\n",
      "('chip', 'pancakes')\n",
      "('dai', 'laew')\n",
      "('alyson', 'kenward')\n",
      "('outt', 'goodthing')\n",
      "('operation', 'ashy')\n",
      "('ashy', 'larry')\n",
      "('leaking', 'fuckyouweather')\n",
      "('fuckyouweather', 'notchill')\n",
      "('nerves', 'shannon')\n",
      "('shannon', 'freeman')\n",
      "('brightness', 'unreasonable')\n",
      "('burberry', 'scarf')\n",
      "('andy', 'gibson')\n",
      "('phones', 'blvd')\n",
      "('mentally', 'motivated')\n",
      "('gesh', 'trynakeepmycolor')\n",
      "('tom', 'petty')\n",
      "('norfolkheatwave', 'edp')\n",
      "('edp', 'pinkun')\n",
      "('flamin', 'dhis')\n",
      "('kylebuice', 'posted')\n",
      "('viewing', 'apartments')\n",
      "('spanish', 'speeking')\n",
      "('thunderrrr', '\\ncan')\n",
      "('www', 'chaqoo')\n",
      "('ppppaying', 'fffffor')\n",
      "('\\xa0\\xa0', 'jokes\\xa0by')\n",
      "('jokes\\xa0by', 'adeleapril\\xa0')\n",
      "('airfield', 'sealed')\n",
      "('kmox', 'stlwx')\n",
      "('stocked', 'cabinet')\n",
      "('dw', 'relief')\n",
      "('et', 'bbcamerica')\n",
      "('orange', 'kitty')\n",
      "('hype', 'expecting')\n",
      "('siren', 'testing')\n",
      "('eric', 'pff')\n",
      "('vice', 'versa')\n",
      "('versa', 'onli')\n",
      "('awwh', 'kwl')\n",
      "('rockhardnipples', 'glasscuttingnipples')\n",
      "('glasscuttingnipples', 'nipplenipples')\n",
      "('nipplenipples', 'nipple')\n",
      "('marginally', 'successful')\n",
      "('hittin', 'gateway')\n",
      "('clan', 'craaazy')\n",
      "('dawns', 'bleak')\n",
      "('dailyhaiku', 'aprnum')\n",
      "('aprnum', 'haiku')\n",
      "('valve', 'marry')\n",
      "('drizzle', 'whataletdown')\n",
      "('nu', 'theta')\n",
      "('rainingagain', 'fitblog')\n",
      "('ol', 'wiscansin')\n",
      "('causes', 'alex')\n",
      "('fiery', 'rage')\n",
      "('foss', 'manor')\n",
      "('luke', 'donald')\n",
      "('optimistically', 'applied')\n",
      "('muddy', 'muck')\n",
      "('wrecked', 'ihatebugs')\n",
      "('erik', 'spoelstra')\n",
      "('ate', 'brownie')\n",
      "('corstens', 'countdown')\n",
      "('thooooooose', 'buys')\n",
      "('ruh', 'roh')\n",
      "('roh', '{rt')\n",
      "('become', 'schizophrenic')\n",
      "('wildfire', 'outbreak')\n",
      "Average Accuracy when ignore absent wors is : 0.8162162162162162\n",
      "Average Accuracy when incoporating absent wors is : 0.8148648648648649\n"
     ]
    }
   ],
   "source": [
    "top_number = 100\n",
    "PMI = calculate_PMI(tweet_dataframe, top_number)\n",
    "print (\"Top 100 sticky terms: \")\n",
    "for terms in PMI.keys():\n",
    "    print (terms)\n",
    "total_error_ignore_absent,total_error_incorporate_absent = calculate_accuracy(tweet_dataframe,PMI)\n",
    "print (\"Average Accuracy when ignore absent wors is : \" + str(1-np.mean(total_error_ignore_absent)))\n",
    "print (\"Average Accuracy when incoporating absent wors is : \" + str(1-np.mean(total_error_incorporate_absent)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get top <font color =\"red\">200 </font>PMI of sticky terms and accuracy after adding them as additional features in Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 200 sticky terms: \n",
      "('sb', 'xlv')\n",
      "('mormon', 'heroin')\n",
      "('board', 'retreat')\n",
      "('listing', 'appt')\n",
      "('gurrl', 'lemme')\n",
      "('hats', 'coats')\n",
      "('fuckkkk', 'thaaaa')\n",
      "('\\nboo', 'ed')\n",
      "('pegged', 'wannanewcar')\n",
      "('wannanewcar', 'sorryxterra')\n",
      "('subra', 'inet')\n",
      "('grrrrrrrrr', 'lazyass')\n",
      "('buzzzzing', 'battleoffife')\n",
      "('childrens', 'parties')\n",
      "('parties', 'difficult')\n",
      "('mankato', 'benefiting')\n",
      "('benefiting', 'leep')\n",
      "('comparison', 'goodbyebluesky')\n",
      "('total', 'horseshit')\n",
      "('electrical', 'appliances')\n",
      "('easily', 'amused')\n",
      "('amused', 'perhaps')\n",
      "('perhaps', 'bite')\n",
      "('siesta', 'key')\n",
      "('key', 'headn')\n",
      "('effing', 'snowflakes')\n",
      "('fort', 'collins')\n",
      "('conditioner', 'kicks')\n",
      "('latest', 'piano')\n",
      "('piano', 'composition')\n",
      "('homie', 'tj')\n",
      "('chip', 'pancakes')\n",
      "('dai', 'laew')\n",
      "('alyson', 'kenward')\n",
      "('outt', 'goodthing')\n",
      "('operation', 'ashy')\n",
      "('ashy', 'larry')\n",
      "('leaking', 'fuckyouweather')\n",
      "('fuckyouweather', 'notchill')\n",
      "('nerves', 'shannon')\n",
      "('shannon', 'freeman')\n",
      "('brightness', 'unreasonable')\n",
      "('burberry', 'scarf')\n",
      "('andy', 'gibson')\n",
      "('phones', 'blvd')\n",
      "('mentally', 'motivated')\n",
      "('gesh', 'trynakeepmycolor')\n",
      "('tom', 'petty')\n",
      "('norfolkheatwave', 'edp')\n",
      "('edp', 'pinkun')\n",
      "('flamin', 'dhis')\n",
      "('kylebuice', 'posted')\n",
      "('viewing', 'apartments')\n",
      "('spanish', 'speeking')\n",
      "('thunderrrr', '\\ncan')\n",
      "('www', 'chaqoo')\n",
      "('ppppaying', 'fffffor')\n",
      "('\\xa0\\xa0', 'jokes\\xa0by')\n",
      "('jokes\\xa0by', 'adeleapril\\xa0')\n",
      "('airfield', 'sealed')\n",
      "('kmox', 'stlwx')\n",
      "('stocked', 'cabinet')\n",
      "('dw', 'relief')\n",
      "('et', 'bbcamerica')\n",
      "('orange', 'kitty')\n",
      "('hype', 'expecting')\n",
      "('siren', 'testing')\n",
      "('eric', 'pff')\n",
      "('vice', 'versa')\n",
      "('versa', 'onli')\n",
      "('awwh', 'kwl')\n",
      "('rockhardnipples', 'glasscuttingnipples')\n",
      "('glasscuttingnipples', 'nipplenipples')\n",
      "('nipplenipples', 'nipple')\n",
      "('marginally', 'successful')\n",
      "('hittin', 'gateway')\n",
      "('clan', 'craaazy')\n",
      "('dawns', 'bleak')\n",
      "('dailyhaiku', 'aprnum')\n",
      "('aprnum', 'haiku')\n",
      "('valve', 'marry')\n",
      "('drizzle', 'whataletdown')\n",
      "('nu', 'theta')\n",
      "('rainingagain', 'fitblog')\n",
      "('ol', 'wiscansin')\n",
      "('causes', 'alex')\n",
      "('fiery', 'rage')\n",
      "('foss', 'manor')\n",
      "('luke', 'donald')\n",
      "('optimistically', 'applied')\n",
      "('muddy', 'muck')\n",
      "('wrecked', 'ihatebugs')\n",
      "('erik', 'spoelstra')\n",
      "('ate', 'brownie')\n",
      "('corstens', 'countdown')\n",
      "('thooooooose', 'buys')\n",
      "('ruh', 'roh')\n",
      "('roh', '{rt')\n",
      "('become', 'schizophrenic')\n",
      "('wildfire', 'outbreak')\n",
      "('linen', 'cargo')\n",
      "('dosnt', 'decribe')\n",
      "('mistake', 'bots')\n",
      "('teamfollowback', 'tfb')\n",
      "('tfb', 'taf')\n",
      "('bicycle', 'commuting')\n",
      "('\\nsincerely', 'yours')\n",
      "('perfectscarymoviesetup', 'uhoh')\n",
      "('freddy', 'garcia')\n",
      "('box', '\\n\\nhappy')\n",
      "('puddles', '\\ngrey')\n",
      "('bluesy', 'broke')\n",
      "('chain', 'fronted')\n",
      "('devy', 'mtfam')\n",
      "('qb', 'newsflash')\n",
      "('louisvilleweather', 'severeweather')\n",
      "('unsuspecting', 'heads')\n",
      "('sumbrero', 'tobiasthegreat')\n",
      "('sinus’', 'loathe')\n",
      "('accounts', 'filed')\n",
      "('dramatically', 'diminishes')\n",
      "('hairstyling', 'options')\n",
      "('cf', 'iswear')\n",
      "('whack', 'anywho')\n",
      "('hysteria', 'weatherwimps')\n",
      "('basset', 'hound')\n",
      "('anxiety', 'attacks')\n",
      "('groggy', 'doe')\n",
      "('vaucluse', 'yls')\n",
      "('yls', 'braving')\n",
      "('faster', 'shark')\n",
      "('shark', 'shaped')\n",
      "('esta', 'enferma')\n",
      "('orige', 'empty')\n",
      "('cuffin', 'screw')\n",
      "('vernal', 'equinox')\n",
      "('eeeeeeee', 'mofo')\n",
      "('yinepu', 'tweets')\n",
      "('strolling', 'mixes')\n",
      "('mixes', 'niptini')\n",
      "('niptini', 'garnish')\n",
      "('prius', 'packs')\n",
      "('packs', 'punch')\n",
      "('fayetteville', 'priced')\n",
      "('crow', 'sheets')\n",
      "('mii', 'casa')\n",
      "('unfair', 'considering')\n",
      "('obpic', 'perfectly')\n",
      "('gracias', 'dios')\n",
      "('seer', 'sucker')\n",
      "('altho', 'mayb')\n",
      "('rafting', 'westwater')\n",
      "('bull', 'iishh')\n",
      "('palmer', 'wood')\n",
      "('skegs', 'skeg')\n",
      "('yippee', 'sarcastic')\n",
      "('fairly', 'disappointing')\n",
      "('positive', 'compliments')\n",
      "('twister', 'hittheditch')\n",
      "('snuggle', 'partner')\n",
      "('bg', 'wishmycarwoulddriveitself')\n",
      "('mjahzah', 'al')\n",
      "('twenty', 'bucks')\n",
      "('final', 'offer')\n",
      "('captains', 'meetings')\n",
      "('killa', 'bees')\n",
      "('bees', 'flyin')\n",
      "('nov', 'dec')\n",
      "('fckin', 'bakin')\n",
      "('towns', 'named')\n",
      "('bonfires', 'blazing')\n",
      "('hanginginthere', 'truckin')\n",
      "('truckin', 'enjoyingthesun')\n",
      "('veinte', 'oreo')\n",
      "('oreo', 'shake')\n",
      "('tullys', 'helllla')\n",
      "('ono', 'hawaiian')\n",
      "('hawaiian', 'cafe')\n",
      "('kanak', 'attack')\n",
      "('attack', 'catered')\n",
      "('laughlin', 'ranch')\n",
      "('heating', 'purposes')\n",
      "('inner', 'harbor')\n",
      "('rib', 'tacos')\n",
      "('mr', 'teddy')\n",
      "('web', 'site')\n",
      "('cornhole', 'tournaments')\n",
      "('mighty', 'margs')\n",
      "('killer', 'ipas')\n",
      "('burlington', 'ontario')\n",
      "('bahaha', 'suckstosuck')\n",
      "('center', 'marriott')\n",
      "('rick', 'ross')\n",
      "('western', 'lakes')\n",
      "('lakes', 'cumbria')\n",
      "('cumbria', 'breathtaking')\n",
      "('breathtaking', 'scenary')\n",
      "('photography', 'projects')\n",
      "('pr', 'photoshoots')\n",
      "('perksofparadise', 'lovemylife')\n",
      "Average Accuracy when ignore absent wors is : 0.8162162162162162\n",
      "Average Accuracy when incoporating absent wors is : 0.8148648648648649\n"
     ]
    }
   ],
   "source": [
    "top_number = 200\n",
    "PMI = calculate_PMI(tweet_dataframe, top_number)\n",
    "print (\"Top 200 sticky terms: \")\n",
    "for terms in PMI.keys():\n",
    "    print (terms)\n",
    "total_error_ignore_absent,total_error_incorporate_absent = calculate_accuracy(tweet_dataframe,PMI)\n",
    "print (\"Average Accuracy when ignore absent wors is : \" + str(1-np.mean(total_error_ignore_absent)))\n",
    "print (\"Average Accuracy when incoporating absent wors is : \" + str(1-np.mean(total_error_incorporate_absent)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get top <font color =\"red\">500 </font>PMI of sticky terms and accuracy after adding them as additional features in Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 500 sticky terms: \n",
      "('sb', 'xlv')\n",
      "('mormon', 'heroin')\n",
      "('board', 'retreat')\n",
      "('listing', 'appt')\n",
      "('gurrl', 'lemme')\n",
      "('hats', 'coats')\n",
      "('fuckkkk', 'thaaaa')\n",
      "('\\nboo', 'ed')\n",
      "('pegged', 'wannanewcar')\n",
      "('wannanewcar', 'sorryxterra')\n",
      "('subra', 'inet')\n",
      "('grrrrrrrrr', 'lazyass')\n",
      "('buzzzzing', 'battleoffife')\n",
      "('childrens', 'parties')\n",
      "('parties', 'difficult')\n",
      "('mankato', 'benefiting')\n",
      "('benefiting', 'leep')\n",
      "('comparison', 'goodbyebluesky')\n",
      "('total', 'horseshit')\n",
      "('electrical', 'appliances')\n",
      "('easily', 'amused')\n",
      "('amused', 'perhaps')\n",
      "('perhaps', 'bite')\n",
      "('siesta', 'key')\n",
      "('key', 'headn')\n",
      "('effing', 'snowflakes')\n",
      "('fort', 'collins')\n",
      "('conditioner', 'kicks')\n",
      "('latest', 'piano')\n",
      "('piano', 'composition')\n",
      "('homie', 'tj')\n",
      "('chip', 'pancakes')\n",
      "('dai', 'laew')\n",
      "('alyson', 'kenward')\n",
      "('outt', 'goodthing')\n",
      "('operation', 'ashy')\n",
      "('ashy', 'larry')\n",
      "('leaking', 'fuckyouweather')\n",
      "('fuckyouweather', 'notchill')\n",
      "('nerves', 'shannon')\n",
      "('shannon', 'freeman')\n",
      "('brightness', 'unreasonable')\n",
      "('burberry', 'scarf')\n",
      "('andy', 'gibson')\n",
      "('phones', 'blvd')\n",
      "('mentally', 'motivated')\n",
      "('gesh', 'trynakeepmycolor')\n",
      "('tom', 'petty')\n",
      "('norfolkheatwave', 'edp')\n",
      "('edp', 'pinkun')\n",
      "('flamin', 'dhis')\n",
      "('kylebuice', 'posted')\n",
      "('viewing', 'apartments')\n",
      "('spanish', 'speeking')\n",
      "('thunderrrr', '\\ncan')\n",
      "('www', 'chaqoo')\n",
      "('ppppaying', 'fffffor')\n",
      "('\\xa0\\xa0', 'jokes\\xa0by')\n",
      "('jokes\\xa0by', 'adeleapril\\xa0')\n",
      "('airfield', 'sealed')\n",
      "('kmox', 'stlwx')\n",
      "('stocked', 'cabinet')\n",
      "('dw', 'relief')\n",
      "('et', 'bbcamerica')\n",
      "('orange', 'kitty')\n",
      "('hype', 'expecting')\n",
      "('siren', 'testing')\n",
      "('eric', 'pff')\n",
      "('vice', 'versa')\n",
      "('versa', 'onli')\n",
      "('awwh', 'kwl')\n",
      "('rockhardnipples', 'glasscuttingnipples')\n",
      "('glasscuttingnipples', 'nipplenipples')\n",
      "('nipplenipples', 'nipple')\n",
      "('marginally', 'successful')\n",
      "('hittin', 'gateway')\n",
      "('clan', 'craaazy')\n",
      "('dawns', 'bleak')\n",
      "('dailyhaiku', 'aprnum')\n",
      "('aprnum', 'haiku')\n",
      "('valve', 'marry')\n",
      "('drizzle', 'whataletdown')\n",
      "('nu', 'theta')\n",
      "('rainingagain', 'fitblog')\n",
      "('ol', 'wiscansin')\n",
      "('causes', 'alex')\n",
      "('fiery', 'rage')\n",
      "('foss', 'manor')\n",
      "('luke', 'donald')\n",
      "('optimistically', 'applied')\n",
      "('muddy', 'muck')\n",
      "('wrecked', 'ihatebugs')\n",
      "('erik', 'spoelstra')\n",
      "('ate', 'brownie')\n",
      "('corstens', 'countdown')\n",
      "('thooooooose', 'buys')\n",
      "('ruh', 'roh')\n",
      "('roh', '{rt')\n",
      "('become', 'schizophrenic')\n",
      "('wildfire', 'outbreak')\n",
      "('linen', 'cargo')\n",
      "('dosnt', 'decribe')\n",
      "('mistake', 'bots')\n",
      "('teamfollowback', 'tfb')\n",
      "('tfb', 'taf')\n",
      "('bicycle', 'commuting')\n",
      "('\\nsincerely', 'yours')\n",
      "('perfectscarymoviesetup', 'uhoh')\n",
      "('freddy', 'garcia')\n",
      "('box', '\\n\\nhappy')\n",
      "('puddles', '\\ngrey')\n",
      "('bluesy', 'broke')\n",
      "('chain', 'fronted')\n",
      "('devy', 'mtfam')\n",
      "('qb', 'newsflash')\n",
      "('louisvilleweather', 'severeweather')\n",
      "('unsuspecting', 'heads')\n",
      "('sumbrero', 'tobiasthegreat')\n",
      "('sinus’', 'loathe')\n",
      "('accounts', 'filed')\n",
      "('dramatically', 'diminishes')\n",
      "('hairstyling', 'options')\n",
      "('cf', 'iswear')\n",
      "('whack', 'anywho')\n",
      "('hysteria', 'weatherwimps')\n",
      "('basset', 'hound')\n",
      "('anxiety', 'attacks')\n",
      "('groggy', 'doe')\n",
      "('vaucluse', 'yls')\n",
      "('yls', 'braving')\n",
      "('faster', 'shark')\n",
      "('shark', 'shaped')\n",
      "('esta', 'enferma')\n",
      "('orige', 'empty')\n",
      "('cuffin', 'screw')\n",
      "('vernal', 'equinox')\n",
      "('eeeeeeee', 'mofo')\n",
      "('yinepu', 'tweets')\n",
      "('strolling', 'mixes')\n",
      "('mixes', 'niptini')\n",
      "('niptini', 'garnish')\n",
      "('prius', 'packs')\n",
      "('packs', 'punch')\n",
      "('fayetteville', 'priced')\n",
      "('crow', 'sheets')\n",
      "('mii', 'casa')\n",
      "('unfair', 'considering')\n",
      "('obpic', 'perfectly')\n",
      "('gracias', 'dios')\n",
      "('seer', 'sucker')\n",
      "('altho', 'mayb')\n",
      "('rafting', 'westwater')\n",
      "('bull', 'iishh')\n",
      "('palmer', 'wood')\n",
      "('skegs', 'skeg')\n",
      "('yippee', 'sarcastic')\n",
      "('fairly', 'disappointing')\n",
      "('positive', 'compliments')\n",
      "('twister', 'hittheditch')\n",
      "('snuggle', 'partner')\n",
      "('bg', 'wishmycarwoulddriveitself')\n",
      "('mjahzah', 'al')\n",
      "('twenty', 'bucks')\n",
      "('final', 'offer')\n",
      "('captains', 'meetings')\n",
      "('killa', 'bees')\n",
      "('bees', 'flyin')\n",
      "('nov', 'dec')\n",
      "('fckin', 'bakin')\n",
      "('towns', 'named')\n",
      "('bonfires', 'blazing')\n",
      "('hanginginthere', 'truckin')\n",
      "('truckin', 'enjoyingthesun')\n",
      "('veinte', 'oreo')\n",
      "('oreo', 'shake')\n",
      "('tullys', 'helllla')\n",
      "('ono', 'hawaiian')\n",
      "('hawaiian', 'cafe')\n",
      "('kanak', 'attack')\n",
      "('attack', 'catered')\n",
      "('laughlin', 'ranch')\n",
      "('heating', 'purposes')\n",
      "('inner', 'harbor')\n",
      "('rib', 'tacos')\n",
      "('mr', 'teddy')\n",
      "('web', 'site')\n",
      "('cornhole', 'tournaments')\n",
      "('mighty', 'margs')\n",
      "('killer', 'ipas')\n",
      "('burlington', 'ontario')\n",
      "('bahaha', 'suckstosuck')\n",
      "('center', 'marriott')\n",
      "('rick', 'ross')\n",
      "('western', 'lakes')\n",
      "('lakes', 'cumbria')\n",
      "('cumbria', 'breathtaking')\n",
      "('breathtaking', 'scenary')\n",
      "('photography', 'projects')\n",
      "('pr', 'photoshoots')\n",
      "('perksofparadise', 'lovemylife')\n",
      "('aday', 'followngain')\n",
      "('followngain', 'followordie')\n",
      "('followordie', 'followte')\n",
      "('followte', 'nnum')\n",
      "('nnum', 'followorswallow')\n",
      "('followorswallow', 'retweetthis')\n",
      "('retweetthis', 'fnum')\n",
      "('camden', 'yards')\n",
      "('habs', 'sox')\n",
      "('sox', 'angels')\n",
      "('twilight', 'zone')\n",
      "('houstondynamo', 'gameday')\n",
      "('unblemished', 'divine')\n",
      "('footage', 'dslr')\n",
      "('toooooo', 'arrriveeeeeeeeeeee')\n",
      "('lime', 'avenue')\n",
      "('thomsons', 'lobsters')\n",
      "('popular', 'science')\n",
      "('whenevers', 'clever')\n",
      "('clever', 'chicas')\n",
      "('katie', 'kat')\n",
      "('guiding', 'snowshoe')\n",
      "('remains', 'stiff')\n",
      "('hahah', 'noo')\n",
      "('election', 'conservative')\n",
      "('conservative', 'nonum')\n",
      "('nonum', 'av')\n",
      "('seattle’s', 'magnuson')\n",
      "('fenum', 'lz')\n",
      "('watchn', 'dha')\n",
      "('ship', 'canal')\n",
      "('original', 'songs')\n",
      "('karl', 'remus')\n",
      "('daii', 'buh')\n",
      "('boone', 'roubaix')\n",
      "('anniversary', 'ily')\n",
      "('ily', 'bby')\n",
      "('stellar', 'website')\n",
      "('pitter', 'pater')\n",
      "('lotsa', 'comedy')\n",
      "('uneeda', 'burger')\n",
      "('gresh', 'motorsports')\n",
      "('worm', 'composting')\n",
      "('troubleshoot', 'sw')\n",
      "('perfectparty', '\\nnum')\n",
      "('~~~', 'silverspoon')\n",
      "('fried', 'brie')\n",
      "('maggie', 'pickle')\n",
      "('cltnews', 'gradual')\n",
      "('koko', 'brown')\n",
      "('backcountry', 'skiers')\n",
      "('grab', 'typhoon')\n",
      "('dosequis', 'poolparty')\n",
      "('giant', 'pot')\n",
      "('organizing~', 'choose')\n",
      "('goldie', 'locks')\n",
      "('leads', 'uni')\n",
      "('essential', 'gumby')\n",
      "('gumby', 'episodes')\n",
      "('ticket', 'sales')\n",
      "('accompany', 'tornadic')\n",
      "('resemble', 'breasts')\n",
      "('chagrin', 'soakinupthesun')\n",
      "('chili', 'bluemangroup')\n",
      "('criss', 'angel')\n",
      "('hamstring', 'fe')\n",
      "('university', 'ave')\n",
      "('palo', 'alto')\n",
      "('colorful', 'lies')\n",
      "('erases', 'hangover')\n",
      "('woodinville', 'tasting')\n",
      "('kenny', 'wallace')\n",
      "('willow', 'grove')\n",
      "('shad', 'boner')\n",
      "('oregon', 'dozes')\n",
      "('added', 'spf')\n",
      "('four', 'separate')\n",
      "('separate', 'seasons')\n",
      "('hungry', 'thn')\n",
      "('queenstowns', 'superb')\n",
      "('cesar', 'chavez')\n",
      "('chavez', 'plaza')\n",
      "('plaza', 'altoarizona')\n",
      "('doorless', 'shawty')\n",
      "('shawty', 'divorced')\n",
      "('wrangler', 'hehehehe')\n",
      "('yuenglings', 'betting')\n",
      "('betting', 'keenland')\n",
      "('shoals', 'huntsville')\n",
      "('trek', 'hq')\n",
      "('ambien', 'slept')\n",
      "('arrested', 'developments')\n",
      "('netflix', 'instant')\n",
      "('instant', 'sotired')\n",
      "('vie', 'en')\n",
      "('boost', 'morale')\n",
      "('morale', 'heifers')\n",
      "('skyrocketed', 'cny')\n",
      "('colleges', 'chapel')\n",
      "('avlwx', 'avlreview')\n",
      "('erodes', 'perspective')\n",
      "('housework', 'jobs')\n",
      "('openwindow', 'noshirt')\n",
      "('admiring', 'fjords')\n",
      "('providence', 'rhode')\n",
      "('freedom', 'momentnum')\n",
      "('mt', 'rainier')\n",
      "('starwars', 'hyper')\n",
      "('scottsdale', 'smallworld')\n",
      "('bumpin', 'flocka')\n",
      "('flocka', 'shakin')\n",
      "('hottie', 'parade')\n",
      "('liberty', 'mutual')\n",
      "('mutual', 'legends')\n",
      "('vodka', 'sauce')\n",
      "('gearsnum', 'beta')\n",
      "('radiohead', 'sharpies')\n",
      "('penn', 'station')\n",
      "('originalwettshirtcontest', 'polish')\n",
      "('eats', 'h')\n",
      "('emerging', 'triumphant')\n",
      "('records', 'highlight')\n",
      "('highlight', 'reinhart')\n",
      "('reinhart', 'relays')\n",
      "('hamock', 'sp')\n",
      "('sp', 'sip')\n",
      "('sip', 'lemon')\n",
      "('lemon', 'aid')\n",
      "('newspaper', 'readin')\n",
      "('camel', 'spiders')\n",
      "('medieval', 'knights')\n",
      "('hook', 'norton')\n",
      "('norton', 'brass')\n",
      "('extended', 'length')\n",
      "('yess', 'sware')\n",
      "('sware', 'ali')\n",
      "('koolaid', 'pitcher')\n",
      "('neil', 'lennon')\n",
      "('lennon', 'postman')\n",
      "('weheat', 'timbs')\n",
      "('rollcoasters', 'sixflags')\n",
      "('huntington', 'downs')\n",
      "('planting', 'raised')\n",
      "('byrd', 'costume')\n",
      "('lynnwood', 'harley')\n",
      "('yuk', 'bolee')\n",
      "('lo', 'blm')\n",
      "('blm', 'pgi')\n",
      "('pgi', 'chel')\n",
      "('greencastle', 'metropolitan')\n",
      "('eden', 'crab')\n",
      "('socks\\xa0for', 'buc')\n",
      "('easterbrunch', 'toofulltoofunction')\n",
      "('deluxe', 'greatest')\n",
      "('cumberland', 'speedway')\n",
      "('breezewaxs', 'ep')\n",
      "('cookouts', 'jumping')\n",
      "('powhite', 'tollbooth')\n",
      "('credit', 'rva')\n",
      "('ygn', 'gang')\n",
      "('roast', 'dinners')\n",
      "('youth', 'churches')\n",
      "('richard', 'hawley')\n",
      "('trueloves', 'gutter')\n",
      "('carne', 'asada')\n",
      "('forrest', 'sauv')\n",
      "('sauv', 'blanc')\n",
      "('arnold', 'palmers')\n",
      "('bros', 'sienna')\n",
      "('pork', 'loin')\n",
      "('bayleys', 'lobster')\n",
      "('lobster', 'pound')\n",
      "('christos', 'anesti')\n",
      "('diet', 'cheerwine')\n",
      "('henry', 'ford')\n",
      "('megamind', 'snuggles')\n",
      "('jolly', 'rancher')\n",
      "('slice', 'backhand')\n",
      "('backhand', 'lean')\n",
      "('eastover', 'jew')\n",
      "('cuteness', 'overload')\n",
      "('practice', 'ucla')\n",
      "('barn', 'newbarn')\n",
      "('lb', 'roasting')\n",
      "('\\n\\nsunshine', '\\n\\nwarm')\n",
      "('\\n\\nguinness', '\\n\\nkid')\n",
      "('acoustic', 'guitar')\n",
      "('guitar', '\\n\\nlife')\n",
      "('practically', 'inevitable')\n",
      "('curriculum', 'packet')\n",
      "('rich', 'pickings')\n",
      "('culinary', 'lavender')\n",
      "('seattles', 'warmest')\n",
      "('hamradio', 'hamr')\n",
      "('hop', 'aboard')\n",
      "('aboard', 'historic')\n",
      "('deer', 'cooldown')\n",
      "('racquets', 'lacoste')\n",
      "('sneakers', 'rayban')\n",
      "('rayban', 'aviators')\n",
      "('aviators', 'amwearing')\n",
      "('eastermorning', 'consisted')\n",
      "('jane', 'eyre')\n",
      "('ers', 'kicked')\n",
      "('brian', 'macmillan')\n",
      "('laws', 'farm')\n",
      "('beachweekend', '\\nwonderful')\n",
      "('wonderufl', 'experience')\n",
      "('unusual', 'number')\n",
      "('rabbits', 'chasing')\n",
      "('alfresco', 'dining')\n",
      "('scuba', 'bot')\n",
      "('amongst', 'collected')\n",
      "('collected', 'cand')\n",
      "('shrimp', 'steak')\n",
      "('steak', 'liquor')\n",
      "('robert', 'cohen')\n",
      "('steve', 'lyttle')\n",
      "('ft', 'lauderdale')\n",
      "('doug', 'lindsay')\n",
      "('lindsay', 'cltwx')\n",
      "('cltwx', 'ncwx')\n",
      "('whoever', 'arranged')\n",
      "('follows', 'behind')\n",
      "('crazy\\n', 'el')\n",
      "('el', 'shaddai')\n",
      "('waterfront', 'homes')\n",
      "('reach', 'macclesfield')\n",
      "('nws', 'tucson')\n",
      "('dave', 'murray')\n",
      "('towards', 'mineral')\n",
      "('mineral', 'wells')\n",
      "('rob', 'martin')\n",
      "('continues', 'throught')\n",
      "('friendlys', 'express')\n",
      "('dito', 'subra')\n",
      "('pink', 'floyds')\n",
      "('lonely', 'teamsingle')\n",
      "('signal', 'grrr')\n",
      "('barking', 'cough')\n",
      "('belly', 'wifi')\n",
      "('wifi', 'connection')\n",
      "('connection', 'down=')\n",
      "('rubbish', 'ihateexams')\n",
      "('runner', 'beans')\n",
      "('widespread', 'multi')\n",
      "('adele', 'pandora')\n",
      "('seruz', 'oo')\n",
      "('marshall', 'blanket')\n",
      "('abtnum', 'pay')\n",
      "('alll', 'fricken')\n",
      "('crying', 'tears')\n",
      "('hotorcold', 'makeupyourmind')\n",
      "('biggest', 'pos')\n",
      "('homes', 'flattened')\n",
      "('flattened', 'whntnum')\n",
      "('ski', 'areas')\n",
      "('rattled', 'nerves')\n",
      "('freeman', 'signing')\n",
      "('friggin', 'loudness')\n",
      "('unreasonable', 'timing')\n",
      "('okoboji', 'bound')\n",
      "('blvd', 'matches')\n",
      "('gun', 'range')\n",
      "('finest', 'parkas')\n",
      "('album', 'leaf')\n",
      "('petty', 'sort')\n",
      "('shootin', 'regardless')\n",
      "('i’m', 'hearing')\n",
      "('tower', 'devastating')\n",
      "('sealed', 'indefinitely')\n",
      "('campus', 'quad')\n",
      "('hissy', 'fit')\n",
      "('pre', 'taped')\n",
      "('memphis', 'period')\n",
      "('umbrella', 'yo')\n",
      "('force', 'closing')\n",
      "('location', 'scouting')\n",
      "('haircut', 'lookingonthebrightside')\n",
      "('toe', 'nails')\n",
      "('springy', 'pink')\n",
      "('hint', 'sends')\n",
      "('active', 'volcano')\n",
      "('dc', 'weathersad')\n",
      "('hky', 'hated')\n",
      "('fly', 'greatt')\n",
      "('da…', 'cont')\n",
      "('mvc', 'womens')\n",
      "('christs', 'sake')\n",
      "('sout…', 'cont')\n",
      "('swing', 'causes')\n",
      "('today…my', 'goal')\n",
      "('thousand', 'suns')\n",
      "('applied', 'factor')\n",
      "('widow', 'spider')\n",
      "('foolish', 'man~')\n",
      "('overit', 'samething')\n",
      "('emotional', 'issues')\n",
      "('vip', 'lounge')\n",
      "('prepare', 'yourselves')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy when ignore absent wors is : 0.8162162162162162\n",
      "Average Accuracy when incoporating absent wors is : 0.8148648648648649\n"
     ]
    }
   ],
   "source": [
    "top_number = 500\n",
    "PMI = calculate_PMI(tweet_dataframe, top_number)\n",
    "print (\"Top 500 sticky terms: \")\n",
    "for terms in PMI.keys():\n",
    "    print (terms)\n",
    "total_error_ignore_absent,total_error_incorporate_absent = calculate_accuracy(tweet_dataframe,PMI)\n",
    "print (\"Average Accuracy when ignore absent wors is : \" + str(1-np.mean(total_error_ignore_absent)))\n",
    "print (\"Average Accuracy when incoporating absent wors is : \" + str(1-np.mean(total_error_incorporate_absent)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report For Part 2\n",
    "    After adding sticky terms, the accuracy of Naive Bayes Model decreases. However, accuracy doesn't change although sticky terms added into model changed. It may caused by same training/test split randomization. Also, without regrading absent words, the accuracy is higher than incorporating absent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
